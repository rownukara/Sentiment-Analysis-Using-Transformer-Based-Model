{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "source": [
        "pip install transformers"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "NOGRjh7QYQ4h",
        "outputId": "622b92b3-a43d-4de2-d906-2cd54e360ef1"
      },
      "execution_count": 29,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: transformers in /usr/local/lib/python3.10/dist-packages (4.31.0)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.10/dist-packages (from transformers) (3.12.2)\n",
            "Requirement already satisfied: huggingface-hub<1.0,>=0.14.1 in /usr/local/lib/python3.10/dist-packages (from transformers) (0.16.4)\n",
            "Requirement already satisfied: numpy>=1.17 in /usr/local/lib/python3.10/dist-packages (from transformers) (1.22.4)\n",
            "Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.10/dist-packages (from transformers) (23.1)\n",
            "Requirement already satisfied: pyyaml>=5.1 in /usr/local/lib/python3.10/dist-packages (from transformers) (6.0.1)\n",
            "Requirement already satisfied: regex!=2019.12.17 in /usr/local/lib/python3.10/dist-packages (from transformers) (2022.10.31)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.10/dist-packages (from transformers) (2.27.1)\n",
            "Requirement already satisfied: tokenizers!=0.11.3,<0.14,>=0.11.1 in /usr/local/lib/python3.10/dist-packages (from transformers) (0.13.3)\n",
            "Requirement already satisfied: safetensors>=0.3.1 in /usr/local/lib/python3.10/dist-packages (from transformers) (0.3.1)\n",
            "Requirement already satisfied: tqdm>=4.27 in /usr/local/lib/python3.10/dist-packages (from transformers) (4.65.0)\n",
            "Requirement already satisfied: fsspec in /usr/local/lib/python3.10/dist-packages (from huggingface-hub<1.0,>=0.14.1->transformers) (2023.6.0)\n",
            "Requirement already satisfied: typing-extensions>=3.7.4.3 in /usr/local/lib/python3.10/dist-packages (from huggingface-hub<1.0,>=0.14.1->transformers) (4.7.1)\n",
            "Requirement already satisfied: urllib3<1.27,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests->transformers) (1.26.16)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests->transformers) (2023.5.7)\n",
            "Requirement already satisfied: charset-normalizer~=2.0.0 in /usr/local/lib/python3.10/dist-packages (from requests->transformers) (2.0.12)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests->transformers) (3.4)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "import pandas as pd\n",
        "import tensorflow as tf\n",
        "from transformers import BertTokenizer, TFBertForSequenceClassification"
      ],
      "metadata": {
        "id": "6u6wBcCNYSBn"
      },
      "execution_count": 30,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Load the dataset\n",
        "# Assuming the dataset is in a file named 'dataset.xlr'\n",
        "data = pd.read_excel('/content/Cricket.xlsx')"
      ],
      "metadata": {
        "id": "gzjxz5snYWGb"
      },
      "execution_count": 31,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Split the dataset into input texts and corresponding labels\n",
        "texts = data['Text'].values\n",
        "labels = data['Polarity'].values"
      ],
      "metadata": {
        "id": "-JGrvMuqYcb9"
      },
      "execution_count": 32,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Load the BanglaBERT tokenizer\n",
        "tokenizer = BertTokenizer.from_pretrained('sagorsarker/bangla-bert-base')"
      ],
      "metadata": {
        "id": "Yv8MyvKsYfVM"
      },
      "execution_count": 33,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Tokenize the input texts\n",
        "encoded_texts = tokenizer(texts.tolist(), padding=True, truncation=True, return_tensors='tf')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "JIxnAU5ZYjBV",
        "outputId": "0afde99c-3762-4fb2-b0f0-e361f534b867"
      },
      "execution_count": 34,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Asking to truncate to max_length but no maximum length is provided and the model has no predefined maximum length. Default to no truncation.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Convert labels to integers (positive: 1, negative: 0)\n",
        "labels = np.array([1 if label == 'positive' else 0 for label in labels])"
      ],
      "metadata": {
        "id": "eBjBEBSeYxf8"
      },
      "execution_count": 35,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Split the data into training and validation sets\n",
        "train_size = int(0.8 * len(encoded_texts['input_ids']))\n",
        "train_inputs = {key: val[:train_size] for key, val in encoded_texts.items()}\n",
        "train_labels = labels[:train_size]\n",
        "\n",
        "val_inputs = {key: val[train_size:] for key, val in encoded_texts.items()}\n",
        "val_labels = labels[train_size:]"
      ],
      "metadata": {
        "id": "OTwjxglIZyEp"
      },
      "execution_count": 39,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Load the pre-trained BanglaBERT model for sequence classification\n",
        "model = TFBertForSequenceClassification.from_pretrained('sagorsarker/bangla-bert-base', num_labels=2)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "n16LZYB5Z2l1",
        "outputId": "07dc75fd-8a84-4f92-ee92-8d2160d5cb42"
      },
      "execution_count": 40,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "All PyTorch model weights were used when initializing TFBertForSequenceClassification.\n",
            "\n",
            "Some weights or buffers of the TF 2.0 model TFBertForSequenceClassification were not initialized from the PyTorch model and are newly initialized: ['classifier.weight', 'classifier.bias']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Compile the model\n",
        "model.compile(optimizer=tf.keras.optimizers.Adam(learning_rate=2e-5), loss=tf.keras.losses.SparseCategoricalCrossentropy(from_logits=True), metrics=['accuracy'])"
      ],
      "metadata": {
        "id": "DYYpWsQQZ_EE"
      },
      "execution_count": 41,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Train the model\n",
        "model.fit(train_inputs, train_labels, validation_data=(val_inputs, val_labels), epochs=5, batch_size=32)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "LbjTqSZeaJLk",
        "outputId": "3af1473b-ee48-4e42-9bf8-72a27abc7ce0"
      },
      "execution_count": 42,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1/5\n",
            "75/75 [==============================] - 1911s 25s/step - loss: 0.4771 - accuracy: 0.7955 - val_loss: 0.2952 - val_accuracy: 0.8943\n",
            "Epoch 2/5\n",
            "75/75 [==============================] - 1801s 24s/step - loss: 0.3315 - accuracy: 0.8547 - val_loss: 0.3111 - val_accuracy: 0.8624\n",
            "Epoch 3/5\n",
            "75/75 [==============================] - 1778s 24s/step - loss: 0.2010 - accuracy: 0.9257 - val_loss: 0.4071 - val_accuracy: 0.8356\n",
            "Epoch 4/5\n",
            "75/75 [==============================] - 1806s 24s/step - loss: 0.1259 - accuracy: 0.9559 - val_loss: 0.3935 - val_accuracy: 0.8607\n",
            "Epoch 5/5\n",
            "75/75 [==============================] - 1780s 24s/step - loss: 0.0837 - accuracy: 0.9736 - val_loss: 0.4306 - val_accuracy: 0.8490\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<keras.callbacks.History at 0x78c5d86e4700>"
            ]
          },
          "metadata": {},
          "execution_count": 42
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Evaluate the model on test data\n",
        "loss, accuracy = model.evaluate(val_inputs, val_labels)\n",
        "print(\"Validation Accuracy:\", accuracy)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "f5Z3IRtoanUh",
        "outputId": "d0e14cbe-47ac-4e49-fd1c-771f8f405e23"
      },
      "execution_count": 43,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "19/19 [==============================] - 141s 7s/step - loss: 0.4306 - accuracy: 0.8490\n",
            "Validation Accuracy: 0.8489933013916016\n"
          ]
        }
      ]
    }
  ]
}